{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to your DataCamp project audition! This notebook must be filled out and vetted before a contract can be signed and you can start creating your project.\n",
    "\n",
    "The first step is forking the repository in which this notebook lives. After that, there are two parts to be completed in this notebook:\n",
    "\n",
    "- **Project information**:  The title of the project, a project description, etc.\n",
    "\n",
    "- **Project introduction**: The three first text and code cells that will form the introduction of your project.\n",
    "\n",
    "When complete, please email the link to your forked repo to projects@datacamp.com with the email subject line _DataCamp project audition_. If you have any questions, please reach out to projects@datacamp.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project title**: Check your lab QC data! Is everything in limit?  \n",
    "\n",
    "**Name:** Deniz Ilhan Topcu\n",
    "\n",
    "**Email address associated with your DataCamp account:** ditopcu@gmail.com\n",
    "\n",
    "**GitHub username:** ditopcu\n",
    "\n",
    "**Project description**: \n",
    "\n",
    "How can clinical laboratories be sure about their results? Statistical **Quality Control** (QC) methods are helping laboratories every day for ensuring the required quality. But evaluating lots of quality control results can be overwhelming. Data science tools could help us to facilitate this process, once again!  \n",
    "In this project, we will investigate laboratory QC data using data import, data wrangling, and visualization tools in R. We’re going to calculate QC statistics and plot Levey-Jennings charts and apply basic [_“Westgard Rules”._  ](https://www.westgard.com/mltirule.htm)\n",
    "\n",
    "Completing [“Introduction to the Tidyverse course” ](https://www.datacamp.com/courses/introduction-to-the-tidyverse)\n",
    "or experience with `dplyr` and `ggplot2` packages is recommended. Also, familiarity with data importing, writing functions and basic functional programming skills will be helpful.  \n",
    "  \n",
    "  \n",
    "We will examine two level QC results for 10 different biochemistry analytes in this project. The dataset comprises simulated QC results based on real clinical laboratory data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "***Note: nothing needs to be filled out in this cell. It is simply setting up the template cells below.***\n",
    "\n",
    "The final output of a DataCamp project looks like a blog post: pairs of text and code cells that tell a story about data. The text is written from the perspective of the data analyst and *not* from the perspective of an instructor on DataCamp. So, for this blog post intro, all you need to do is pretend like you're writing a blog post -- forget the part about instructors and students.\n",
    "\n",
    "Below you'll see the structure of a DataCamp project: a series of \"tasks\" where each task consists of a title, a **single** text cell, and a **single** code cell. There are 8-12 tasks in a project and each task can have up to 10 lines of code. What you need to do:\n",
    "1. Read through the template structure.\n",
    "2. As best you can, divide your project as it is currently visualized in your mind into tasks.\n",
    "3. Fill out the template structure for the first three tasks of your project.\n",
    "\n",
    "As you are completing each task, you may wish to consult the project notebook format in our [documentation](https://instructor-support.datacamp.com/projects/datacamp-projects-jupyter-notebook). Only the `@context` and `@solution` cells are relevant to this audition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding and Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical laboratory tests such as serum glucose, HDL, Iron, etc. are using to evaluate the condition of a patient and they are important for clinical decision.  \n",
    "\n",
    "![Clinical Lab](img/visual.png)\n",
    "\n",
    "\n",
    "Quality control (QC) is a process to periodically examine these tests measurement procedure and verify that it is performing according to predefined specifications. There are two main errors for measurement: inaccuracy and imprecision.  \n",
    "\n",
    "\n",
    "![Measure](img/measure03.png)\n",
    "\n",
    "\n",
    "**Accuracy** is used to describe the closeness of a measurement to the true value.  **Precision** is the closeness of agreement between repeated measurements of a sample.  \n",
    "Laboratories are using internal quality control (IQC) procedures to assess their imprecision (random error). Traditionally, IQC uses sample materials with assigned values and IQC results are evaluated continuously in relation to these known values.   \n",
    "\n",
    "To understand these procedures and evaluate imprecision, we are going to inspect QC results. \n",
    "We have QC results for 10 different analytes as separate CSV files and one CSV file for predefined specifications for these 10 tests. We are going to import these files, then evaluate precision as in terms of mean, standard deviation. Finally, we plot Levey-Jennings charts using these statistics.  \n",
    "\n",
    "Let's start with data importing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tidyverse and lubridate package\n",
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "\n",
    "\n",
    "# Check current directory for .csv files with ends \"_QC_results\"\n",
    "list.files(\"datasets/\", pattern = \"_QC_results.csv\")\n",
    "\n",
    "# Test import: import Albumin_QC_results.csv file into test_read using read_csv and inspect results.\n",
    "test_read <- read_csv(\"datasets/Albumin_QC_results.csv\")\n",
    "\n",
    "glimpse(test_read)\n",
    "head(test_read)\n",
    "\n",
    "\n",
    "\n",
    "# Import again  Albumin_QC_results.csv file now into glucose_qc with datetime parsing and\n",
    "#  factor conversion\n",
    "\n",
    "\n",
    "albumin_qc <- read_csv(\"datasets/Albumin_QC_results.csv\") %>%\n",
    "  mutate(result_date = ymd_hms(result_date)) %>%\n",
    "  mutate(lot_number = as.factor(lot_number), level = as.factor(level))\n",
    "\n",
    "#Inspect results\n",
    "glimpse(albumin_qc)\n",
    "head(albumin_qc)\n",
    "\n",
    "\n",
    "# Import all test specifications(test_specs.csv) into test_performance_data and inspect\n",
    "test_performance_data <- read_csv(\"datasets/test_specs.csv\")\n",
    "\n",
    "glimpse(test_performance_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Data Using Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just read glucose QC results. But there are 9 files to go! We want to evaluate all biochemistry data.  \n",
    "Should we copy&past above code?\n",
    "\n",
    "No. There must be more elegant way. Let's define a function and use this function to read all files. To do this:\n",
    "\n",
    "* Define a reader function: We need a function which takes file name as parameter and return a tibble. While defining function we should add col types (col_types) while using read_csv file in a function. This will a) Ensure we are reading files correctly b) Suppress warning messages\n",
    "\n",
    "* Test function\n",
    "\n",
    "* Use some kind of loops to read all files: We will use purr::map family functions\n",
    "\n",
    "Now we are going to complete these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert above code into a function which accepts file_name and returns tibble\n",
    "\n",
    "qc_result_reader <- function(file_name) {\n",
    "  read_csv(file_name,\n",
    "    col_types = cols(\n",
    "      new_device_name = col_character(),\n",
    "      lot_number = col_integer(),\n",
    "      level = col_character(),\n",
    "      result_date = col_character(),\n",
    "      test_code = col_character(),\n",
    "      result = col_double(),\n",
    "      unit = col_character(),\n",
    "      name = col_character()\n",
    "    )\n",
    "  ) %>%\n",
    "    mutate(result_date = ymd_hms(result_date)) %>%\n",
    "    mutate(lot_number = as.factor(lot_number), level = as.factor(level))\n",
    "}\n",
    "\n",
    "\n",
    "# Test function with Glucose_QC_results.csv and import glucose data into albumin_qc\n",
    "\n",
    "glucose_qc <- qc_result_reader(\"datasets/Glucose_QC_results.csv\")\n",
    "\n",
    "\n",
    "glimpse(glucose_qc)\n",
    "head(glucose_qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Our function is working!  Now we have a function to read our QC Files. \n",
    "\n",
    "_Hint:_ We provide `col_types` to ensure we are reading columns correctly. We can also use specific `col_` instead of additional  mutate's. For example: `result_date = col_datetime(format = \"%Y.%m.%d %H:%M:%S\")` instead of `lubridate::ymd_hms`. This can be fast in big data sets.  \n",
    "  \n",
    "We can use `map_df` from `purrr` package to read all files and combine into one single tibble. `purrr::map` family functions provides simple interfaces for repetitive tasks. \n",
    "\n",
    "\n",
    "`map_df` function also accepts `.id` parameter to create additional variable which contains index name. In this case our file name.  (use ?map_df for details)\n",
    "\n",
    "To obtain test names from file names we can use regex.\n",
    "\n",
    "After that we are ready to do real analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get file names and paths using full.names argument\n",
    "\n",
    "file_list <- list.files(path = \"datasets/\", pattern = \"_QC_results.csv\", full.names = TRUE)\n",
    "\n",
    "# Inspect file_list\n",
    "file_list\n",
    "\n",
    "\n",
    "# use regex to remove path and \"_QC_results\" tag and obtain test names. Set these  names to use in map_df\n",
    "test_names <- file_list %>%\n",
    "  str_extract(\"(?<=\\\\/)(.*?)(?=\\\\.)\") %>% # extracts text between / and .\n",
    "  str_replace(\"_QC_results\", \"\") # removes _QC_results\n",
    "\n",
    "names(file_list) <- test_names\n",
    "\n",
    "# Read all files in file_list\n",
    "all_test_qc <- map_df(file_list, qc_result_reader, .id = \"test_name\")\n",
    "\n",
    "# inspect data\n",
    "glimpse(all_test_qc)\n",
    "head(all_test_qc)\n",
    "\n",
    "# count test and levels\n",
    "count(all_test_qc, test_name, level)\n",
    "\n",
    "\n",
    "# Now we are ready to join data and do some statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stop here! Only the three first tasks. :)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
