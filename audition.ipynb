{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to your DataCamp project audition! This notebook must be filled out and vetted before a contract can be signed and you can start creating your project.\n",
    "\n",
    "The first step is forking the repository in which this notebook lives. After that, there are two parts to be completed in this notebook:\n",
    "\n",
    "- **Project information**:  The title of the project, a project description, etc.\n",
    "\n",
    "- **Project introduction**: The three first text and code cells that will form the introduction of your project.\n",
    "\n",
    "When complete, please email the link to your forked repo to projects@datacamp.com with the email subject line _DataCamp project audition_. If you have any questions, please reach out to projects@datacamp.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project title**: Check your lab QC data! Is everything in limit?  \n",
    "\n",
    "**Name:** Deniz Ilhan Topcu\n",
    "\n",
    "**Email address associated with your DataCamp account:** ditopcu@gmail.com\n",
    "\n",
    "**GitHub username:** ditopcu\n",
    "\n",
    "**Project description**: \n",
    "\n",
    "How can clinical laboratories be sure about their results? Statistical **Quality Control** (QC) methods are helping laboratories every day for ensuring the required quality. But evaluating lots of quality control results can be overwhelming. Data science tools could help us to facilitate this process, once again!  \n",
    "In this project, we will investigate laboratory QC data using data import, data wrangling, and visualization tools in R. We’re going to calculate QC statistics and plot Levey-Jennings charts and apply basic [_“Westgard Rules”._  ](https://www.westgard.com/mltirule.htm)\n",
    "\n",
    "Completing [“Introduction to the Tidyverse course” ](https://www.datacamp.com/courses/introduction-to-the-tidyverse)\n",
    "or experience with `dplyr` and `ggplot2` packages is recommended. Also, familiarity with data importing, writing functions and basic functional programming skills will be helpful.  \n",
    "  \n",
    "  \n",
    "We will examine two level QC results for 10 different biochemistry analytes in this project. The dataset comprises simulated QC results based on real clinical laboratory data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "***Note: nothing needs to be filled out in this cell. It is simply setting up the template cells below.***\n",
    "\n",
    "The final output of a DataCamp project looks like a blog post: pairs of text and code cells that tell a story about data. The text is written from the perspective of the data analyst and *not* from the perspective of an instructor on DataCamp. So, for this blog post intro, all you need to do is pretend like you're writing a blog post -- forget the part about instructors and students.\n",
    "\n",
    "Below you'll see the structure of a DataCamp project: a series of \"tasks\" where each task consists of a title, a **single** text cell, and a **single** code cell. There are 8-12 tasks in a project and each task can have up to 10 lines of code. What you need to do:\n",
    "1. Read through the template structure.\n",
    "2. As best you can, divide your project as it is currently visualized in your mind into tasks.\n",
    "3. Fill out the template structure for the first three tasks of your project.\n",
    "\n",
    "As you are completing each task, you may wish to consult the project notebook format in our [documentation](https://instructor-support.datacamp.com/projects/datacamp-projects-jupyter-notebook). Only the `@context` and `@solution` cells are relevant to this audition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding and Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical laboratory tests such as serum glucose, HDL, Iron, etc. are using to evaluate the condition of a patient and they are important for clinical decision.  \n",
    "\n",
    "![Clinical Lab](img/visual.png)\n",
    "\n",
    "\n",
    "Quality control (QC) is a process to periodically examine these tests measurement procedure and verify that it is performing according to predefined specifications. There are two main errors for measurement: inaccuracy and imprecision.  \n",
    "\n",
    "\n",
    "![Measure](img/measure03.png)\n",
    "\n",
    "\n",
    "**Accuracy** is used to describe the closeness of a measurement to the true value.  **Precision** is the closeness of agreement between repeated measurements of a sample.  \n",
    "Laboratories are using internal quality control (IQC) procedures to assess their imprecision (random error). Traditionally, IQC uses sample materials with assigned values and IQC results are evaluated continuously in relation to these known values.   \n",
    "\n",
    "To understand these procedures and evaluate imprecision, we are going to inspect QC results. \n",
    "We have QC results for 10 different analytes as separate CSV files and one CSV file for predefined specifications for these 10 tests. We are going to import these files, then evaluate precision as in terms of mean, standard deviation. Finally, we plot Levey-Jennings charts using these statistics.  \n",
    "\n",
    "Let's start with data importing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Albumin_QC_results.csv'</li>\n",
       "\t<li>'ALT_QC_results.csv'</li>\n",
       "\t<li>'Amylase_QC_results.csv'</li>\n",
       "\t<li>'Calcium_QC_results.csv'</li>\n",
       "\t<li>'Chloride_QC_results.csv'</li>\n",
       "\t<li>'Glucose_QC_results.csv'</li>\n",
       "\t<li>'HDL Cholesterol_QC_results.csv'</li>\n",
       "\t<li>'Potassium_QC_results.csv'</li>\n",
       "\t<li>'Sodium_QC_results.csv'</li>\n",
       "\t<li>'Total Cholesterol_QC_results.csv'</li>\n",
       "\t<li>'Total Protein_QC_results.csv'</li>\n",
       "\t<li>'Triglyceride_QC_results.csv'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Albumin\\_QC\\_results.csv'\n",
       "\\item 'ALT\\_QC\\_results.csv'\n",
       "\\item 'Amylase\\_QC\\_results.csv'\n",
       "\\item 'Calcium\\_QC\\_results.csv'\n",
       "\\item 'Chloride\\_QC\\_results.csv'\n",
       "\\item 'Glucose\\_QC\\_results.csv'\n",
       "\\item 'HDL Cholesterol\\_QC\\_results.csv'\n",
       "\\item 'Potassium\\_QC\\_results.csv'\n",
       "\\item 'Sodium\\_QC\\_results.csv'\n",
       "\\item 'Total Cholesterol\\_QC\\_results.csv'\n",
       "\\item 'Total Protein\\_QC\\_results.csv'\n",
       "\\item 'Triglyceride\\_QC\\_results.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Albumin_QC_results.csv'\n",
       "2. 'ALT_QC_results.csv'\n",
       "3. 'Amylase_QC_results.csv'\n",
       "4. 'Calcium_QC_results.csv'\n",
       "5. 'Chloride_QC_results.csv'\n",
       "6. 'Glucose_QC_results.csv'\n",
       "7. 'HDL Cholesterol_QC_results.csv'\n",
       "8. 'Potassium_QC_results.csv'\n",
       "9. 'Sodium_QC_results.csv'\n",
       "10. 'Total Cholesterol_QC_results.csv'\n",
       "11. 'Total Protein_QC_results.csv'\n",
       "12. 'Triglyceride_QC_results.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Albumin_QC_results.csv\"           \"ALT_QC_results.csv\"              \n",
       " [3] \"Amylase_QC_results.csv\"           \"Calcium_QC_results.csv\"          \n",
       " [5] \"Chloride_QC_results.csv\"          \"Glucose_QC_results.csv\"          \n",
       " [7] \"HDL Cholesterol_QC_results.csv\"   \"Potassium_QC_results.csv\"        \n",
       " [9] \"Sodium_QC_results.csv\"            \"Total Cholesterol_QC_results.csv\"\n",
       "[11] \"Total Protein_QC_results.csv\"     \"Triglyceride_QC_results.csv\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  new_device_name = col_character(),\n",
      "  lot_number = col_integer(),\n",
      "  level = col_character(),\n",
      "  result_date = col_character(),\n",
      "  test_code = col_character(),\n",
      "  result = col_double(),\n",
      "  unit = col_character(),\n",
      "  name = col_character()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 376\n",
      "Variables: 8\n",
      "$ new_device_name <chr> \"Analyser 001\", \"Analyser 001\", \"Analyser 001\", \"An...\n",
      "$ lot_number      <int> 123456, 123456, 123456, 123456, 123456, 123456, 123...\n",
      "$ level           <chr> \"002\", \"001\", \"001\", \"002\", \"001\", \"002\", \"002\", \"0...\n",
      "$ result_date     <chr> \"2018.01.01 08:13:00\", \"2018.01.01 08:21:00\", \"2018...\n",
      "$ test_code       <chr> \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t0...\n",
      "$ result          <dbl> 4.17, 3.28, 3.24, 4.13, 3.23, 4.16, 4.18, 3.22, 3.2...\n",
      "$ unit            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n",
      "$ name            <chr> \"Turner, Cameron\", \"Cordova, Rocky\", \"Cordova, Rock...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>new_device_name</th><th scope=col>lot_number</th><th scope=col>level</th><th scope=col>result_date</th><th scope=col>test_code</th><th scope=col>result</th><th scope=col>unit</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018.01.01 08:13:00</td><td>t001               </td><td>4.17               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018.01.01 08:21:00</td><td>t001               </td><td>3.28               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018.01.02 09:20:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018.01.02 09:28:00</td><td>t001               </td><td>4.13               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018.01.03 09:21:00</td><td>t001               </td><td>3.23               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018.01.03 09:21:00</td><td>t001               </td><td>4.16               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " new\\_device\\_name & lot\\_number & level & result\\_date & test\\_code & result & unit & name\\\\\n",
       "\\hline\n",
       "\t Analyser 001        & 123456              & 002                 & 2018.01.01 08:13:00 & t001                & 4.17                & NA                  & Turner, Cameron    \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018.01.01 08:21:00 & t001                & 3.28                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018.01.02 09:20:00 & t001                & 3.24                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018.01.02 09:28:00 & t001                & 4.13                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018.01.03 09:21:00 & t001                & 3.23                & NA                  & Acres, Valerie     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018.01.03 09:21:00 & t001                & 4.16                & NA                  & Cordova, Rocky     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "new_device_name | lot_number | level | result_date | test_code | result | unit | name | \n",
       "|---|---|---|---|---|---|\n",
       "| Analyser 001        | 123456              | 002                 | 2018.01.01 08:13:00 | t001                | 4.17                | NA                  | Turner, Cameron     | \n",
       "| Analyser 001        | 123456              | 001                 | 2018.01.01 08:21:00 | t001                | 3.28                | NA                  | Cordova, Rocky      | \n",
       "| Analyser 001        | 123456              | 001                 | 2018.01.02 09:20:00 | t001                | 3.24                | NA                  | Cordova, Rocky      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018.01.02 09:28:00 | t001                | 4.13                | NA                  | Ramundo, Dustyn     | \n",
       "| Analyser 001        | 123456              | 001                 | 2018.01.03 09:21:00 | t001                | 3.23                | NA                  | Acres, Valerie      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018.01.03 09:21:00 | t001                | 4.16                | NA                  | Cordova, Rocky      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  new_device_name lot_number level result_date         test_code result unit\n",
       "1 Analyser 001    123456     002   2018.01.01 08:13:00 t001      4.17   NA  \n",
       "2 Analyser 001    123456     001   2018.01.01 08:21:00 t001      3.28   NA  \n",
       "3 Analyser 001    123456     001   2018.01.02 09:20:00 t001      3.24   NA  \n",
       "4 Analyser 001    123456     002   2018.01.02 09:28:00 t001      4.13   NA  \n",
       "5 Analyser 001    123456     001   2018.01.03 09:21:00 t001      3.23   NA  \n",
       "6 Analyser 001    123456     002   2018.01.03 09:21:00 t001      4.16   NA  \n",
       "  name           \n",
       "1 Turner, Cameron\n",
       "2 Cordova, Rocky \n",
       "3 Cordova, Rocky \n",
       "4 Ramundo, Dustyn\n",
       "5 Acres, Valerie \n",
       "6 Cordova, Rocky "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  new_device_name = col_character(),\n",
      "  lot_number = col_integer(),\n",
      "  level = col_character(),\n",
      "  result_date = col_character(),\n",
      "  test_code = col_character(),\n",
      "  result = col_double(),\n",
      "  unit = col_character(),\n",
      "  name = col_character()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 376\n",
      "Variables: 8\n",
      "$ new_device_name <chr> \"Analyser 001\", \"Analyser 001\", \"Analyser 001\", \"An...\n",
      "$ lot_number      <fct> 123456, 123456, 123456, 123456, 123456, 123456, 123...\n",
      "$ level           <fct> 002, 001, 001, 002, 001, 002, 002, 001, 001, 002, 0...\n",
      "$ result_date     <dttm> 2018-01-01 08:13:00, 2018-01-01 08:21:00, 2018-01-...\n",
      "$ test_code       <chr> \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t0...\n",
      "$ result          <dbl> 4.17, 3.28, 3.24, 4.13, 3.23, 4.16, 4.18, 3.22, 3.2...\n",
      "$ unit            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n",
      "$ name            <chr> \"Turner, Cameron\", \"Cordova, Rocky\", \"Cordova, Rock...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>new_device_name</th><th scope=col>lot_number</th><th scope=col>level</th><th scope=col>result_date</th><th scope=col>test_code</th><th scope=col>result</th><th scope=col>unit</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-01 08:13:00</td><td>t001               </td><td>4.17               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-01 08:21:00</td><td>t001               </td><td>3.28               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-02 09:20:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-02 09:28:00</td><td>t001               </td><td>4.13               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>3.23               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>4.16               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " new\\_device\\_name & lot\\_number & level & result\\_date & test\\_code & result & unit & name\\\\\n",
       "\\hline\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-01 08:13:00 & t001                & 4.17                & NA                  & Turner, Cameron    \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-01 08:21:00 & t001                & 3.28                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-02 09:20:00 & t001                & 3.24                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-02 09:28:00 & t001                & 4.13                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-03 09:21:00 & t001                & 3.23                & NA                  & Acres, Valerie     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-03 09:21:00 & t001                & 4.16                & NA                  & Cordova, Rocky     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "new_device_name | lot_number | level | result_date | test_code | result | unit | name | \n",
       "|---|---|---|---|---|---|\n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-01 08:13:00 | t001                | 4.17                | NA                  | Turner, Cameron     | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-01 08:21:00 | t001                | 3.28                | NA                  | Cordova, Rocky      | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-02 09:20:00 | t001                | 3.24                | NA                  | Cordova, Rocky      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-02 09:28:00 | t001                | 4.13                | NA                  | Ramundo, Dustyn     | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-03 09:21:00 | t001                | 3.23                | NA                  | Acres, Valerie      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-03 09:21:00 | t001                | 4.16                | NA                  | Cordova, Rocky      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  new_device_name lot_number level result_date         test_code result unit\n",
       "1 Analyser 001    123456     002   2018-01-01 08:13:00 t001      4.17   NA  \n",
       "2 Analyser 001    123456     001   2018-01-01 08:21:00 t001      3.28   NA  \n",
       "3 Analyser 001    123456     001   2018-01-02 09:20:00 t001      3.24   NA  \n",
       "4 Analyser 001    123456     002   2018-01-02 09:28:00 t001      4.13   NA  \n",
       "5 Analyser 001    123456     001   2018-01-03 09:21:00 t001      3.23   NA  \n",
       "6 Analyser 001    123456     002   2018-01-03 09:21:00 t001      4.16   NA  \n",
       "  name           \n",
       "1 Turner, Cameron\n",
       "2 Cordova, Rocky \n",
       "3 Cordova, Rocky \n",
       "4 Ramundo, Dustyn\n",
       "5 Acres, Valerie \n",
       "6 Cordova, Rocky "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  new_test_name = col_character(),\n",
      "  new_lot_number = col_character(),\n",
      "  man_mean = col_number(),\n",
      "  man_sd = col_character()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 24\n",
      "Variables: 4\n",
      "$ new_test_name  <chr> \"Albumin\", \"Albumin\", \"ALT\", \"ALT\", \"Amylase\", \"Amyl...\n",
      "$ new_lot_number <chr> \"123456-001\", \"123456-002\", \"123456-001\", \"123456-00...\n",
      "$ man_mean       <dbl> 324, 417, 27, 1189, 415, 1225, 579, 1019, 79, 942, 4...\n",
      "$ man_sd         <chr> \"0,072\", \"0,075\", \"1,168\", \"5,65\", \"1,44\", \"3,79\", \"...\n"
     ]
    }
   ],
   "source": [
    "# Load tidyverse and lubridate package\n",
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "\n",
    "# Before importing data let’s start with learning which files we have. Then import glucose results.\n",
    "\n",
    "# Check current directory for .csv files with ends with _QC_results\n",
    "list.files(\"datasets/\", pattern = \"_QC_results.csv\")\n",
    "\n",
    "# Import Albumin_QC_results.csv file into test_read using read_csv and inspect results.\n",
    "\n",
    "\n",
    "  test_read <- read_csv(\"datasets/Albumin_QC_results.csv\")\n",
    "  \n",
    "  glimpse(test_read)\n",
    "  head(test_read)\n",
    "\n",
    "# Import  Glucose.csv file into glucose_qc and inspect results\n",
    "# We have result_date column but it is parsed as character in test_read\n",
    "# Let's fix this using ymd_hms function (?ymd_hms for details )\n",
    "# also convert lot_number, and levels columns into factor\n",
    "\n",
    "\n",
    "albumin_qc <- read_csv(\"datasets/Albumin_QC_results.csv\") %>% \n",
    "    mutate(result_date = ymd_hms(result_date)) %>% \n",
    "    mutate(lot_number = as.factor(lot_number), level = as.factor(level))\n",
    "    \n",
    "glimpse(albumin_qc)\n",
    "head(albumin_qc)\n",
    "\n",
    "\n",
    "# Import all test specifications(test_specs.csv) into test_performance_data and inspect\n",
    "test_performance_data<-read_csv(\"datasets/test_specs.csv\")\n",
    "\n",
    "glimpse(test_performance_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing file using function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just read glucose QC results. But there are 9 files to go! Should we copy&past above code?\n",
    "\n",
    "No. There must be more elegant way. Let's define a function and use this function to read all files. To do this:\n",
    "\n",
    "1. Define a reader function: We need a function which takes file name as parameter and return a tibble. While defining function we should add col types (col_types) while using read_csv file in a function. This will a) Ensure we are reading files correctly b) Suppress warning messages\n",
    "\n",
    "2. Test function\n",
    "\n",
    "3. Use some kind of loops to read all files: We will use purr::map family functions\n",
    "\n",
    "Now we are going to complete first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 376\n",
      "Variables: 8\n",
      "$ new_device_name <chr> \"Analyser 001\", \"Analyser 001\", \"Analyser 001\", \"An...\n",
      "$ lot_number      <fct> 123456, 123456, 123456, 123456, 123456, 123456, 123...\n",
      "$ level           <fct> 002, 001, 001, 002, 002, 001, 001, 002, 001, 002, 0...\n",
      "$ result_date     <dttm> 2018-01-01 08:18:00, 2018-01-01 08:23:00, 2018-01-...\n",
      "$ test_code       <chr> \"t008\", \"t008\", \"t008\", \"t008\", \"t008\", \"t008\", \"t0...\n",
      "$ result          <dbl> 119, 46, 46, 118, 119, 46, 46, 119, 46, 119, 46, 11...\n",
      "$ unit            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n",
      "$ name            <chr> \"Seidel, Sophia\", \"Seidel, Sophia\", \"Acres, Valerie...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>new_device_name</th><th scope=col>lot_number</th><th scope=col>level</th><th scope=col>result_date</th><th scope=col>test_code</th><th scope=col>result</th><th scope=col>unit</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-01 08:18:00</td><td>t008               </td><td>119                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-01 08:23:00</td><td>t008               </td><td> 46                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-02 09:28:00</td><td>t008               </td><td> 46                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-02 09:28:00</td><td>t008               </td><td>118                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-03 09:29:00</td><td>t008               </td><td>119                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-03 09:30:00</td><td>t008               </td><td> 46                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " new\\_device\\_name & lot\\_number & level & result\\_date & test\\_code & result & unit & name\\\\\n",
       "\\hline\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-01 08:18:00 & t008                & 119                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-01 08:23:00 & t008                &  46                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-02 09:28:00 & t008                &  46                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-02 09:28:00 & t008                & 118                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Analyser 001        & 123456              & 002                 & 2018-01-03 09:29:00 & t008                & 119                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Analyser 001        & 123456              & 001                 & 2018-01-03 09:30:00 & t008                &  46                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "new_device_name | lot_number | level | result_date | test_code | result | unit | name | \n",
       "|---|---|---|---|---|---|\n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-01 08:18:00 | t008                | 119                 | NA                  | Seidel, Sophia      | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-01 08:23:00 | t008                |  46                 | NA                  | Seidel, Sophia      | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-02 09:28:00 | t008                |  46                 | NA                  | Acres, Valerie      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-02 09:28:00 | t008                | 118                 | NA                  | Acres, Valerie      | \n",
       "| Analyser 001        | 123456              | 002                 | 2018-01-03 09:29:00 | t008                | 119                 | NA                  | Acres, Valerie      | \n",
       "| Analyser 001        | 123456              | 001                 | 2018-01-03 09:30:00 | t008                |  46                 | NA                  | Ramundo, Dustyn     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  new_device_name lot_number level result_date         test_code result unit\n",
       "1 Analyser 001    123456     002   2018-01-01 08:18:00 t008      119    NA  \n",
       "2 Analyser 001    123456     001   2018-01-01 08:23:00 t008       46    NA  \n",
       "3 Analyser 001    123456     001   2018-01-02 09:28:00 t008       46    NA  \n",
       "4 Analyser 001    123456     002   2018-01-02 09:28:00 t008      118    NA  \n",
       "5 Analyser 001    123456     002   2018-01-03 09:29:00 t008      119    NA  \n",
       "6 Analyser 001    123456     001   2018-01-03 09:30:00 t008       46    NA  \n",
       "  name           \n",
       "1 Seidel, Sophia \n",
       "2 Seidel, Sophia \n",
       "3 Acres, Valerie \n",
       "4 Acres, Valerie \n",
       "5 Acres, Valerie \n",
       "6 Ramundo, Dustyn"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We already know how to read file. Convert these into a function is simple\n",
    "\n",
    "qc_result_reader <- function(file_name) {    \n",
    "    read_csv(file_name, \n",
    "            col_types = cols(  new_device_name = col_character(),\n",
    "                  lot_number = col_integer(),\n",
    "                  level = col_character(),\n",
    "                  result_date = col_character(),\n",
    "                  test_code = col_character(),\n",
    "                  result = col_double(),\n",
    "                  unit = col_character(),\n",
    "                  name = col_character())) %>% \n",
    "    mutate(result_date = ymd_hms(result_date)) %>% \n",
    "    mutate(lot_number = as.factor(lot_number), level = as.factor(level))\n",
    "    \n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Test function with Glucose_QC_results.csv and import glucose data into albumin_qc\n",
    "\n",
    "glucose_qc <- qc_result_reader(\"datasets/Glucose_QC_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "glimpse(glucose_qc)\n",
    "head(glucose_qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Our function is working! \n",
    "  \n",
    "Now we have a function to read our QC Files. \n",
    "\n",
    "Hint: We provide `col_types` to ensure we are reading columns correctly. We can also use specific `col_` instead of additional  mutate's. For example: `result_date = col_datetime(format = \"%Y.%m.%d %H:%M:%S\")` instead of `lubridate::ymd_hms`. This can be fast in big data sets.  \n",
    "  \n",
    "\n",
    "We can use `map_df` from `purrr` package to read all files and combine into one single tibble. `purrr::map` family functions provides simple interfaces for repetitive tasks. \n",
    "\n",
    "\n",
    "`map_df` function also accepts `.id` parameter to create additional variable which contains index name. In this case our file name.  (use ?map_df for details)\n",
    "\n",
    "To obtain test names from file names we can use regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'datasets/Albumin_QC_results.csv'</li>\n",
       "\t<li>'datasets/ALT_QC_results.csv'</li>\n",
       "\t<li>'datasets/Amylase_QC_results.csv'</li>\n",
       "\t<li>'datasets/Calcium_QC_results.csv'</li>\n",
       "\t<li>'datasets/Chloride_QC_results.csv'</li>\n",
       "\t<li>'datasets/Glucose_QC_results.csv'</li>\n",
       "\t<li>'datasets/HDL Cholesterol_QC_results.csv'</li>\n",
       "\t<li>'datasets/Potassium_QC_results.csv'</li>\n",
       "\t<li>'datasets/Sodium_QC_results.csv'</li>\n",
       "\t<li>'datasets/Total Cholesterol_QC_results.csv'</li>\n",
       "\t<li>'datasets/Total Protein_QC_results.csv'</li>\n",
       "\t<li>'datasets/Triglyceride_QC_results.csv'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'datasets/Albumin\\_QC\\_results.csv'\n",
       "\\item 'datasets/ALT\\_QC\\_results.csv'\n",
       "\\item 'datasets/Amylase\\_QC\\_results.csv'\n",
       "\\item 'datasets/Calcium\\_QC\\_results.csv'\n",
       "\\item 'datasets/Chloride\\_QC\\_results.csv'\n",
       "\\item 'datasets/Glucose\\_QC\\_results.csv'\n",
       "\\item 'datasets/HDL Cholesterol\\_QC\\_results.csv'\n",
       "\\item 'datasets/Potassium\\_QC\\_results.csv'\n",
       "\\item 'datasets/Sodium\\_QC\\_results.csv'\n",
       "\\item 'datasets/Total Cholesterol\\_QC\\_results.csv'\n",
       "\\item 'datasets/Total Protein\\_QC\\_results.csv'\n",
       "\\item 'datasets/Triglyceride\\_QC\\_results.csv'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'datasets/Albumin_QC_results.csv'\n",
       "2. 'datasets/ALT_QC_results.csv'\n",
       "3. 'datasets/Amylase_QC_results.csv'\n",
       "4. 'datasets/Calcium_QC_results.csv'\n",
       "5. 'datasets/Chloride_QC_results.csv'\n",
       "6. 'datasets/Glucose_QC_results.csv'\n",
       "7. 'datasets/HDL Cholesterol_QC_results.csv'\n",
       "8. 'datasets/Potassium_QC_results.csv'\n",
       "9. 'datasets/Sodium_QC_results.csv'\n",
       "10. 'datasets/Total Cholesterol_QC_results.csv'\n",
       "11. 'datasets/Total Protein_QC_results.csv'\n",
       "12. 'datasets/Triglyceride_QC_results.csv'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"datasets/Albumin_QC_results.csv\"          \n",
       " [2] \"datasets/ALT_QC_results.csv\"              \n",
       " [3] \"datasets/Amylase_QC_results.csv\"          \n",
       " [4] \"datasets/Calcium_QC_results.csv\"          \n",
       " [5] \"datasets/Chloride_QC_results.csv\"         \n",
       " [6] \"datasets/Glucose_QC_results.csv\"          \n",
       " [7] \"datasets/HDL Cholesterol_QC_results.csv\"  \n",
       " [8] \"datasets/Potassium_QC_results.csv\"        \n",
       " [9] \"datasets/Sodium_QC_results.csv\"           \n",
       "[10] \"datasets/Total Cholesterol_QC_results.csv\"\n",
       "[11] \"datasets/Total Protein_QC_results.csv\"    \n",
       "[12] \"datasets/Triglyceride_QC_results.csv\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>test_name</th><th scope=col>new_device_name</th><th scope=col>lot_number</th><th scope=col>level</th><th scope=col>result_date</th><th scope=col>test_code</th><th scope=col>result</th><th scope=col>unit</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-01 08:13:00</td><td>t001               </td><td>4.17               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-01 08:21:00</td><td>t001               </td><td>3.28               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-02 09:20:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-02 09:28:00</td><td>t001               </td><td>4.13               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>3.23               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>4.16               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-04 09:45:00</td><td>t001               </td><td>4.18               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-04 09:49:00</td><td>t001               </td><td>3.22               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-05 09:23:00</td><td>t001               </td><td>3.23               </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-05 09:31:00</td><td>t001               </td><td>4.19               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-06 09:16:00</td><td>t001               </td><td>3.25               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-06 09:23:00</td><td>t001               </td><td>4.15               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-07 09:55:00</td><td>t001               </td><td>3.20               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-07 09:56:00</td><td>t001               </td><td>4.15               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-08 09:35:00</td><td>t001               </td><td>4.14               </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-08 09:39:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-09 09:07:00</td><td>t001               </td><td>4.08               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-09 09:17:00</td><td>t001               </td><td>3.21               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-10 09:11:00</td><td>t001               </td><td>3.21               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-10 09:15:00</td><td>t001               </td><td>4.13               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-11 09:18:00</td><td>t001               </td><td>4.09               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-11 10:54:00</td><td>t001               </td><td>3.20               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-12 09:18:00</td><td>t001               </td><td>3.21               </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-12 09:19:00</td><td>t001               </td><td>4.11               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-13 09:28:00</td><td>t001               </td><td>4.16               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-13 09:32:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-14 09:24:00</td><td>t001               </td><td>3.22               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-14 09:28:00</td><td>t001               </td><td>4.15               </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-15 08:12:00</td><td>t001               </td><td>3.22               </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-15 08:12:00</td><td>t001               </td><td>4.14               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-23 09:11:00</td><td>t011               </td><td> 57                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-23 09:20:00</td><td>t011               </td><td>153                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-24 10:42:00</td><td>t011               </td><td> 57                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-24 10:54:00</td><td>t011               </td><td>160                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-25 09:13:00</td><td>t011               </td><td> 59                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-25 09:14:00</td><td>t011               </td><td>161                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-26 09:45:00</td><td>t011               </td><td> 62                </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-26 09:51:00</td><td>t011               </td><td>165                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-27 09:21:00</td><td>t011               </td><td>154                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-27 09:23:00</td><td>t011               </td><td> 57                </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-28 09:24:00</td><td>t011               </td><td> 56                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-28 09:25:00</td><td>t011               </td><td>153                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-29 09:46:00</td><td>t011               </td><td>148                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-29 09:47:00</td><td>t011               </td><td> 54                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-06-30 09:11:00</td><td>t011               </td><td>163                </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-06-30 09:17:00</td><td>t011               </td><td> 56                </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-01 09:30:00</td><td>t011               </td><td>154                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-01 09:58:00</td><td>t011               </td><td> 57                </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-02 09:21:00</td><td>t011               </td><td> 61                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-02 09:31:00</td><td>t011               </td><td>158                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-03 09:15:00</td><td>t011               </td><td>150                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-03 09:18:00</td><td>t011               </td><td> 56                </td><td>NA                 </td><td>Seidel, Sophia     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-04 09:02:00</td><td>t011               </td><td>153                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-04 09:06:00</td><td>t011               </td><td> 57                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-05 09:10:00</td><td>t011               </td><td> 58                </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-05 09:11:00</td><td>t011               </td><td>159                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-06 09:23:00</td><td>t011               </td><td>161                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-06 10:29:00</td><td>t011               </td><td> 59                </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-07-07 09:25:00</td><td>t011               </td><td>142                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Triglyceride       </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-07-07 10:56:00</td><td>t011               </td><td> 60                </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " test\\_name & new\\_device\\_name & lot\\_number & level & result\\_date & test\\_code & result & unit & name\\\\\n",
       "\\hline\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-01 08:13:00 & t001                & 4.17                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-01 08:21:00 & t001                & 3.28                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-02 09:20:00 & t001                & 3.24                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-02 09:28:00 & t001                & 4.13                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-03 09:21:00 & t001                & 3.23                & NA                  & Acres, Valerie     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-03 09:21:00 & t001                & 4.16                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-04 09:45:00 & t001                & 4.18                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-04 09:49:00 & t001                & 3.22                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-05 09:23:00 & t001                & 3.23                & NA                  & Seidel, Sophia     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-05 09:31:00 & t001                & 4.19                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-06 09:16:00 & t001                & 3.25                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-06 09:23:00 & t001                & 4.15                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-07 09:55:00 & t001                & 3.20                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-07 09:56:00 & t001                & 4.15                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-08 09:35:00 & t001                & 4.14                & NA                  & Seidel, Sophia     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-08 09:39:00 & t001                & 3.24                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-09 09:07:00 & t001                & 4.08                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-09 09:17:00 & t001                & 3.21                & NA                  & Acres, Valerie     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-10 09:11:00 & t001                & 3.21                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-10 09:15:00 & t001                & 4.13                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-11 09:18:00 & t001                & 4.09                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-11 10:54:00 & t001                & 3.20                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-12 09:18:00 & t001                & 3.21                & NA                  & Seidel, Sophia     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-12 09:19:00 & t001                & 4.11                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-13 09:28:00 & t001                & 4.16                & NA                  & Acres, Valerie     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-13 09:32:00 & t001                & 3.24                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-14 09:24:00 & t001                & 3.22                & NA                  & Acres, Valerie     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-14 09:28:00 & t001                & 4.15                & NA                  & Seidel, Sophia     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-15 08:12:00 & t001                & 3.22                & NA                  & Seidel, Sophia     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-15 08:12:00 & t001                & 4.14                & NA                  & Acres, Valerie     \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-23 09:11:00 & t011                &  57                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-23 09:20:00 & t011                & 153                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-24 10:42:00 & t011                &  57                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-24 10:54:00 & t011                & 160                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-25 09:13:00 & t011                &  59                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-25 09:14:00 & t011                & 161                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-26 09:45:00 & t011                &  62                 & NA                  & Turner, Cameron    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-26 09:51:00 & t011                & 165                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-27 09:21:00 & t011                & 154                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-27 09:23:00 & t011                &  57                 & NA                  & Turner, Cameron    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-28 09:24:00 & t011                &  56                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-28 09:25:00 & t011                & 153                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-29 09:46:00 & t011                & 148                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-29 09:47:00 & t011                &  54                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-06-30 09:11:00 & t011                & 163                 & NA                  & Turner, Cameron    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-06-30 09:17:00 & t011                &  56                 & NA                  & Turner, Cameron    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-01 09:30:00 & t011                & 154                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-01 09:58:00 & t011                &  57                 & NA                  & Turner, Cameron    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-02 09:21:00 & t011                &  61                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-02 09:31:00 & t011                & 158                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-03 09:15:00 & t011                & 150                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-03 09:18:00 & t011                &  56                 & NA                  & Seidel, Sophia     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-04 09:02:00 & t011                & 153                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-04 09:06:00 & t011                &  57                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-05 09:10:00 & t011                &  58                 & NA                  & Cordova, Rocky     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-05 09:11:00 & t011                & 159                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-06 09:23:00 & t011                & 161                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-06 10:29:00 & t011                &  59                 & NA                  & Acres, Valerie     \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 002                 & 2018-07-07 09:25:00 & t011                & 142                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Triglyceride        & Analyser 001        & 123456              & 001                 & 2018-07-07 10:56:00 & t011                &  60                 & NA                  & Ramundo, Dustyn    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "test_name | new_device_name | lot_number | level | result_date | test_code | result | unit | name | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-01 08:13:00 | t001                | 4.17                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-01 08:21:00 | t001                | 3.28                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-02 09:20:00 | t001                | 3.24                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-02 09:28:00 | t001                | 4.13                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-03 09:21:00 | t001                | 3.23                | NA                  | Acres, Valerie      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-03 09:21:00 | t001                | 4.16                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-04 09:45:00 | t001                | 4.18                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-04 09:49:00 | t001                | 3.22                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-05 09:23:00 | t001                | 3.23                | NA                  | Seidel, Sophia      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-05 09:31:00 | t001                | 4.19                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-06 09:16:00 | t001                | 3.25                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-06 09:23:00 | t001                | 4.15                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-07 09:55:00 | t001                | 3.20                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-07 09:56:00 | t001                | 4.15                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-08 09:35:00 | t001                | 4.14                | NA                  | Seidel, Sophia      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-08 09:39:00 | t001                | 3.24                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-09 09:07:00 | t001                | 4.08                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-09 09:17:00 | t001                | 3.21                | NA                  | Acres, Valerie      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-10 09:11:00 | t001                | 3.21                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-10 09:15:00 | t001                | 4.13                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-11 09:18:00 | t001                | 4.09                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-11 10:54:00 | t001                | 3.20                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-12 09:18:00 | t001                | 3.21                | NA                  | Seidel, Sophia      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-12 09:19:00 | t001                | 4.11                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-13 09:28:00 | t001                | 4.16                | NA                  | Acres, Valerie      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-13 09:32:00 | t001                | 3.24                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-14 09:24:00 | t001                | 3.22                | NA                  | Acres, Valerie      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-14 09:28:00 | t001                | 4.15                | NA                  | Seidel, Sophia      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-15 08:12:00 | t001                | 3.22                | NA                  | Seidel, Sophia      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-15 08:12:00 | t001                | 4.14                | NA                  | Acres, Valerie      | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-23 09:11:00 | t011                |  57                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-23 09:20:00 | t011                | 153                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-24 10:42:00 | t011                |  57                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-24 10:54:00 | t011                | 160                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-25 09:13:00 | t011                |  59                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-25 09:14:00 | t011                | 161                 | NA                  | Acres, Valerie      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-26 09:45:00 | t011                |  62                 | NA                  | Turner, Cameron     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-26 09:51:00 | t011                | 165                 | NA                  | Seidel, Sophia      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-27 09:21:00 | t011                | 154                 | NA                  | Seidel, Sophia      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-27 09:23:00 | t011                |  57                 | NA                  | Turner, Cameron     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-28 09:24:00 | t011                |  56                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-28 09:25:00 | t011                | 153                 | NA                  | Acres, Valerie      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-29 09:46:00 | t011                | 148                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-29 09:47:00 | t011                |  54                 | NA                  | Acres, Valerie      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-06-30 09:11:00 | t011                | 163                 | NA                  | Turner, Cameron     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-06-30 09:17:00 | t011                |  56                 | NA                  | Turner, Cameron     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-01 09:30:00 | t011                | 154                 | NA                  | Seidel, Sophia      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-01 09:58:00 | t011                |  57                 | NA                  | Turner, Cameron     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-02 09:21:00 | t011                |  61                 | NA                  | Seidel, Sophia      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-02 09:31:00 | t011                | 158                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-03 09:15:00 | t011                | 150                 | NA                  | Acres, Valerie      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-03 09:18:00 | t011                |  56                 | NA                  | Seidel, Sophia      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-04 09:02:00 | t011                | 153                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-04 09:06:00 | t011                |  57                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-05 09:10:00 | t011                |  58                 | NA                  | Cordova, Rocky      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-05 09:11:00 | t011                | 159                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-06 09:23:00 | t011                | 161                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-06 10:29:00 | t011                |  59                 | NA                  | Acres, Valerie      | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 002                 | 2018-07-07 09:25:00 | t011                | 142                 | NA                  | Ramundo, Dustyn     | \n",
       "| Triglyceride        | Analyser 001        | 123456              | 001                 | 2018-07-07 10:56:00 | t011                |  60                 | NA                  | Ramundo, Dustyn     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     test_name    new_device_name lot_number level result_date        \n",
       "1    Albumin      Analyser 001    123456     002   2018-01-01 08:13:00\n",
       "2    Albumin      Analyser 001    123456     001   2018-01-01 08:21:00\n",
       "3    Albumin      Analyser 001    123456     001   2018-01-02 09:20:00\n",
       "4    Albumin      Analyser 001    123456     002   2018-01-02 09:28:00\n",
       "5    Albumin      Analyser 001    123456     001   2018-01-03 09:21:00\n",
       "6    Albumin      Analyser 001    123456     002   2018-01-03 09:21:00\n",
       "7    Albumin      Analyser 001    123456     002   2018-01-04 09:45:00\n",
       "8    Albumin      Analyser 001    123456     001   2018-01-04 09:49:00\n",
       "9    Albumin      Analyser 001    123456     001   2018-01-05 09:23:00\n",
       "10   Albumin      Analyser 001    123456     002   2018-01-05 09:31:00\n",
       "11   Albumin      Analyser 001    123456     001   2018-01-06 09:16:00\n",
       "12   Albumin      Analyser 001    123456     002   2018-01-06 09:23:00\n",
       "13   Albumin      Analyser 001    123456     001   2018-01-07 09:55:00\n",
       "14   Albumin      Analyser 001    123456     002   2018-01-07 09:56:00\n",
       "15   Albumin      Analyser 001    123456     002   2018-01-08 09:35:00\n",
       "16   Albumin      Analyser 001    123456     001   2018-01-08 09:39:00\n",
       "17   Albumin      Analyser 001    123456     002   2018-01-09 09:07:00\n",
       "18   Albumin      Analyser 001    123456     001   2018-01-09 09:17:00\n",
       "19   Albumin      Analyser 001    123456     001   2018-01-10 09:11:00\n",
       "20   Albumin      Analyser 001    123456     002   2018-01-10 09:15:00\n",
       "21   Albumin      Analyser 001    123456     002   2018-01-11 09:18:00\n",
       "22   Albumin      Analyser 001    123456     001   2018-01-11 10:54:00\n",
       "23   Albumin      Analyser 001    123456     001   2018-01-12 09:18:00\n",
       "24   Albumin      Analyser 001    123456     002   2018-01-12 09:19:00\n",
       "25   Albumin      Analyser 001    123456     002   2018-01-13 09:28:00\n",
       "26   Albumin      Analyser 001    123456     001   2018-01-13 09:32:00\n",
       "27   Albumin      Analyser 001    123456     001   2018-01-14 09:24:00\n",
       "28   Albumin      Analyser 001    123456     002   2018-01-14 09:28:00\n",
       "29   Albumin      Analyser 001    123456     001   2018-01-15 08:12:00\n",
       "30   Albumin      Analyser 001    123456     002   2018-01-15 08:12:00\n",
       "...  ...          ...             ...        ...   ...                \n",
       "4482 Triglyceride Analyser 001    123456     001   2018-06-23 09:11:00\n",
       "4483 Triglyceride Analyser 001    123456     002   2018-06-23 09:20:00\n",
       "4484 Triglyceride Analyser 001    123456     001   2018-06-24 10:42:00\n",
       "4485 Triglyceride Analyser 001    123456     002   2018-06-24 10:54:00\n",
       "4486 Triglyceride Analyser 001    123456     001   2018-06-25 09:13:00\n",
       "4487 Triglyceride Analyser 001    123456     002   2018-06-25 09:14:00\n",
       "4488 Triglyceride Analyser 001    123456     001   2018-06-26 09:45:00\n",
       "4489 Triglyceride Analyser 001    123456     002   2018-06-26 09:51:00\n",
       "4490 Triglyceride Analyser 001    123456     002   2018-06-27 09:21:00\n",
       "4491 Triglyceride Analyser 001    123456     001   2018-06-27 09:23:00\n",
       "4492 Triglyceride Analyser 001    123456     001   2018-06-28 09:24:00\n",
       "4493 Triglyceride Analyser 001    123456     002   2018-06-28 09:25:00\n",
       "4494 Triglyceride Analyser 001    123456     002   2018-06-29 09:46:00\n",
       "4495 Triglyceride Analyser 001    123456     001   2018-06-29 09:47:00\n",
       "4496 Triglyceride Analyser 001    123456     002   2018-06-30 09:11:00\n",
       "4497 Triglyceride Analyser 001    123456     001   2018-06-30 09:17:00\n",
       "4498 Triglyceride Analyser 001    123456     002   2018-07-01 09:30:00\n",
       "4499 Triglyceride Analyser 001    123456     001   2018-07-01 09:58:00\n",
       "4500 Triglyceride Analyser 001    123456     001   2018-07-02 09:21:00\n",
       "4501 Triglyceride Analyser 001    123456     002   2018-07-02 09:31:00\n",
       "4502 Triglyceride Analyser 001    123456     002   2018-07-03 09:15:00\n",
       "4503 Triglyceride Analyser 001    123456     001   2018-07-03 09:18:00\n",
       "4504 Triglyceride Analyser 001    123456     002   2018-07-04 09:02:00\n",
       "4505 Triglyceride Analyser 001    123456     001   2018-07-04 09:06:00\n",
       "4506 Triglyceride Analyser 001    123456     001   2018-07-05 09:10:00\n",
       "4507 Triglyceride Analyser 001    123456     002   2018-07-05 09:11:00\n",
       "4508 Triglyceride Analyser 001    123456     002   2018-07-06 09:23:00\n",
       "4509 Triglyceride Analyser 001    123456     001   2018-07-06 10:29:00\n",
       "4510 Triglyceride Analyser 001    123456     002   2018-07-07 09:25:00\n",
       "4511 Triglyceride Analyser 001    123456     001   2018-07-07 10:56:00\n",
       "     test_code result unit name           \n",
       "1    t001      4.17   NA   Turner, Cameron\n",
       "2    t001      3.28   NA   Cordova, Rocky \n",
       "3    t001      3.24   NA   Cordova, Rocky \n",
       "4    t001      4.13   NA   Ramundo, Dustyn\n",
       "5    t001      3.23   NA   Acres, Valerie \n",
       "6    t001      4.16   NA   Cordova, Rocky \n",
       "7    t001      4.18   NA   Ramundo, Dustyn\n",
       "8    t001      3.22   NA   Ramundo, Dustyn\n",
       "9    t001      3.23   NA   Seidel, Sophia \n",
       "10   t001      4.19   NA   Turner, Cameron\n",
       "11   t001      3.25   NA   Cordova, Rocky \n",
       "12   t001      4.15   NA   Ramundo, Dustyn\n",
       "13   t001      3.20   NA   Cordova, Rocky \n",
       "14   t001      4.15   NA   Ramundo, Dustyn\n",
       "15   t001      4.14   NA   Seidel, Sophia \n",
       "16   t001      3.24   NA   Cordova, Rocky \n",
       "17   t001      4.08   NA   Turner, Cameron\n",
       "18   t001      3.21   NA   Acres, Valerie \n",
       "19   t001      3.21   NA   Turner, Cameron\n",
       "20   t001      4.13   NA   Cordova, Rocky \n",
       "21   t001      4.09   NA   Turner, Cameron\n",
       "22   t001      3.20   NA   Ramundo, Dustyn\n",
       "23   t001      3.21   NA   Seidel, Sophia \n",
       "24   t001      4.11   NA   Ramundo, Dustyn\n",
       "25   t001      4.16   NA   Acres, Valerie \n",
       "26   t001      3.24   NA   Ramundo, Dustyn\n",
       "27   t001      3.22   NA   Acres, Valerie \n",
       "28   t001      4.15   NA   Seidel, Sophia \n",
       "29   t001      3.22   NA   Seidel, Sophia \n",
       "30   t001      4.14   NA   Acres, Valerie \n",
       "...  ...       ...    ...  ...            \n",
       "4482 t011       57    NA   Ramundo, Dustyn\n",
       "4483 t011      153    NA   Ramundo, Dustyn\n",
       "4484 t011       57    NA   Cordova, Rocky \n",
       "4485 t011      160    NA   Ramundo, Dustyn\n",
       "4486 t011       59    NA   Cordova, Rocky \n",
       "4487 t011      161    NA   Acres, Valerie \n",
       "4488 t011       62    NA   Turner, Cameron\n",
       "4489 t011      165    NA   Seidel, Sophia \n",
       "4490 t011      154    NA   Seidel, Sophia \n",
       "4491 t011       57    NA   Turner, Cameron\n",
       "4492 t011       56    NA   Cordova, Rocky \n",
       "4493 t011      153    NA   Acres, Valerie \n",
       "4494 t011      148    NA   Cordova, Rocky \n",
       "4495 t011       54    NA   Acres, Valerie \n",
       "4496 t011      163    NA   Turner, Cameron\n",
       "4497 t011       56    NA   Turner, Cameron\n",
       "4498 t011      154    NA   Seidel, Sophia \n",
       "4499 t011       57    NA   Turner, Cameron\n",
       "4500 t011       61    NA   Seidel, Sophia \n",
       "4501 t011      158    NA   Cordova, Rocky \n",
       "4502 t011      150    NA   Acres, Valerie \n",
       "4503 t011       56    NA   Seidel, Sophia \n",
       "4504 t011      153    NA   Cordova, Rocky \n",
       "4505 t011       57    NA   Cordova, Rocky \n",
       "4506 t011       58    NA   Cordova, Rocky \n",
       "4507 t011      159    NA   Ramundo, Dustyn\n",
       "4508 t011      161    NA   Ramundo, Dustyn\n",
       "4509 t011       59    NA   Acres, Valerie \n",
       "4510 t011      142    NA   Ramundo, Dustyn\n",
       "4511 t011       60    NA   Ramundo, Dustyn"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 4,511\n",
      "Variables: 9\n",
      "$ test_name       <chr> \"Albumin\", \"Albumin\", \"Albumin\", \"Albumin\", \"Albumi...\n",
      "$ new_device_name <chr> \"Analyser 001\", \"Analyser 001\", \"Analyser 001\", \"An...\n",
      "$ lot_number      <fct> 123456, 123456, 123456, 123456, 123456, 123456, 123...\n",
      "$ level           <fct> 002, 001, 001, 002, 001, 002, 002, 001, 001, 002, 0...\n",
      "$ result_date     <dttm> 2018-01-01 08:13:00, 2018-01-01 08:21:00, 2018-01-...\n",
      "$ test_code       <chr> \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t001\", \"t0...\n",
      "$ result          <dbl> 4.17, 3.28, 3.24, 4.13, 3.23, 4.16, 4.18, 3.22, 3.2...\n",
      "$ unit            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n",
      "$ name            <chr> \"Turner, Cameron\", \"Cordova, Rocky\", \"Cordova, Rock...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>test_name</th><th scope=col>new_device_name</th><th scope=col>lot_number</th><th scope=col>level</th><th scope=col>result_date</th><th scope=col>test_code</th><th scope=col>result</th><th scope=col>unit</th><th scope=col>name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-01 08:13:00</td><td>t001               </td><td>4.17               </td><td>NA                 </td><td>Turner, Cameron    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-01 08:21:00</td><td>t001               </td><td>3.28               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-02 09:20:00</td><td>t001               </td><td>3.24               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-02 09:28:00</td><td>t001               </td><td>4.13               </td><td>NA                 </td><td>Ramundo, Dustyn    </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>001                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>3.23               </td><td>NA                 </td><td>Acres, Valerie     </td></tr>\n",
       "\t<tr><td>Albumin            </td><td>Analyser 001       </td><td>123456             </td><td>002                </td><td>2018-01-03 09:21:00</td><td>t001               </td><td>4.16               </td><td>NA                 </td><td>Cordova, Rocky     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " test\\_name & new\\_device\\_name & lot\\_number & level & result\\_date & test\\_code & result & unit & name\\\\\n",
       "\\hline\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-01 08:13:00 & t001                & 4.17                & NA                  & Turner, Cameron    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-01 08:21:00 & t001                & 3.28                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-02 09:20:00 & t001                & 3.24                & NA                  & Cordova, Rocky     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-02 09:28:00 & t001                & 4.13                & NA                  & Ramundo, Dustyn    \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 001                 & 2018-01-03 09:21:00 & t001                & 3.23                & NA                  & Acres, Valerie     \\\\\n",
       "\t Albumin             & Analyser 001        & 123456              & 002                 & 2018-01-03 09:21:00 & t001                & 4.16                & NA                  & Cordova, Rocky     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "test_name | new_device_name | lot_number | level | result_date | test_code | result | unit | name | \n",
       "|---|---|---|---|---|---|\n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-01 08:13:00 | t001                | 4.17                | NA                  | Turner, Cameron     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-01 08:21:00 | t001                | 3.28                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-02 09:20:00 | t001                | 3.24                | NA                  | Cordova, Rocky      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-02 09:28:00 | t001                | 4.13                | NA                  | Ramundo, Dustyn     | \n",
       "| Albumin             | Analyser 001        | 123456              | 001                 | 2018-01-03 09:21:00 | t001                | 3.23                | NA                  | Acres, Valerie      | \n",
       "| Albumin             | Analyser 001        | 123456              | 002                 | 2018-01-03 09:21:00 | t001                | 4.16                | NA                  | Cordova, Rocky      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  test_name new_device_name lot_number level result_date         test_code\n",
       "1 Albumin   Analyser 001    123456     002   2018-01-01 08:13:00 t001     \n",
       "2 Albumin   Analyser 001    123456     001   2018-01-01 08:21:00 t001     \n",
       "3 Albumin   Analyser 001    123456     001   2018-01-02 09:20:00 t001     \n",
       "4 Albumin   Analyser 001    123456     002   2018-01-02 09:28:00 t001     \n",
       "5 Albumin   Analyser 001    123456     001   2018-01-03 09:21:00 t001     \n",
       "6 Albumin   Analyser 001    123456     002   2018-01-03 09:21:00 t001     \n",
       "  result unit name           \n",
       "1 4.17   NA   Turner, Cameron\n",
       "2 3.28   NA   Cordova, Rocky \n",
       "3 3.24   NA   Cordova, Rocky \n",
       "4 4.13   NA   Ramundo, Dustyn\n",
       "5 3.23   NA   Acres, Valerie \n",
       "6 4.16   NA   Cordova, Rocky "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get file names and paths using full.names argument \n",
    "\n",
    "file_list <- list.files(path =\"datasets/\", pattern = \"_QC_results.csv\", full.names = TRUE)\n",
    "\n",
    "# Inspect file_list\n",
    "file_list\n",
    "\n",
    "\n",
    "# use regex to remove path and \"_QC_results\" tag and obtain test names. Set these  names to use in map_df\n",
    "\n",
    "test_names <-  file_list  %>% \n",
    "    str_extract( \"(?<=\\\\/)(.*?)(?=\\\\.)\")  %>% # extracts text between / and .\n",
    "    str_replace(\"_QC_results\", \"\") # removes _QC_results \n",
    "\n",
    "names(file_list) <- test_names\n",
    "\n",
    "# Read all files in file_list\n",
    "all_test_qc <- map_df(file_list, qc_result_reader, .id = \"test_name\")\n",
    "\n",
    "all_test_qc\n",
    "\n",
    "\n",
    "\n",
    "glimpse(all_test_qc)\n",
    "head(all_test_qc)\n",
    "\n",
    "# Now we are ready to join data and do statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stop here! Only the three first tasks. :)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
